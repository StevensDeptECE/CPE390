                           // x0
uint64_t optimized1(uint64_t a) {
   return a*16; // lsl x0, x0, #4
}

uint64_t optimized2(uint64_t a) {
   return a/32; // lsr x0, x0, #5
}

uint64_t optimized2(uint64_t a) {
   return a%32; // and x0, x0, #31 (right answer, 1 clock)
   /*
   crazy, but works!
   mov x1, x0
   lsr x1, x1, #5
   lsl x1, x1, #5
   sub x0, x0, x1

*/
}


uint64_t optimized2(uint64_t a) {
  return 5+6; // mov x0, #11 (right answer
}
  // not great, but we would accept:
  // mov x1, #6
  // add x0, x1, #5

_Z9countonesm:
//  x0=101010101010101010101010101001111001110010100
//     000000000000000000000000000000000000000000001
  mov x2, #0
  mov x3, #64
1:
  and x1, x0, #1
  cmp x1, #0
  beq skip
  add x2, x2, #1
skip:
  lsr  x0, x0, #1 // keep peeling off 1 bit at a time
  subs x3, x3, #1
  bne  1b
  mov x0, x2
  ret

mov x2, #0
1:
  and x1, x0, #1 // x1 = x0 AND 1
  add x2, x2, x1
  lsrs x0, x0, #1 // x0 = x0 >> 1 and set flags
  bne  1b
  mov  x0, x2
  ret
  

6. add only the odd numbers
   x0 = array
   x1 = length

_Z6sumoddPKmm:
   mov  x2, #0 // sum = 0
1:
   ldr  w3, [x0], #4
   tst  w3, #1
   beq  skip
   add  x2, x2, #1
skip:
   subs  x1, #1
   bne   1b
   mov  x0, x2
   ret
   

